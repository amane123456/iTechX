{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLyofE5_d_L0"
      },
      "source": [
        "# Probability for Rendering\n",
        "\n",
        "> This lab intends to familiar you with basic math components in rendering, which should have been covered in *Probability* and *Mathematical Analysis*.\n",
        "\n",
        "Hi! Welcome to Computer Graphics at ShanghaiTech University!\n",
        "In this lab, we'll guide you through some simple yet important factors in building a *Physically-based Render*.\n",
        "It might also serve as an excuse for you to review probability theory.\n",
        "\n",
        "Please execute the following code section to make sure that all dependencies are installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xf9B9IqJd_L1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statistics\n",
        "import math\n",
        "import random\n",
        "\n",
        "import scipy.integrate as integrate\n",
        "import scipy.special as special\n",
        "import scipy.optimize as optimize\n",
        "from scipy.stats import norm\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3Ej2ZJtd_L1"
      },
      "source": [
        "## Estimator Basics\n",
        "\n",
        "You might have heard of the words *biased*, *consistent* or *unbiased* along the way of learning probability.\n",
        "Generally, these words are used to describe *estimators*.\n",
        "**Estimators are random variables**. The reason they are called *estimators* is that, they are used to estimate some *underlying values*.\n",
        "So for arbitrary *estimator*, we can take, for example, expectation and variance on them.\n",
        "Don't worry if you don't understand what do me mean by *underlying values*, by the end of this lab, you'll get a comprehensive understanding of estimator.\n",
        "\n",
        "Here is a simple and incomplete abstraction of normal distribution $\\mathcal{N}(\\mu_i, \\sigma_i^2)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwKjmcvOd_L1"
      },
      "outputs": [],
      "source": [
        "class Normal:\n",
        "    def __init__(self, n: int = 100) -> None:\n",
        "        self.n_ = n  # num samples\n",
        "        self.mu_ = math.sqrt(42)\n",
        "        self.sigma_ = 24\n",
        "        self.samples_ = np.random.normal(self.mu_, self.sigma_, self.n_)\n",
        "\n",
        "    def sample(self) -> float:\n",
        "        self.n_ -= 1\n",
        "        result = self.samples_[self.n_]\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCiMUFv3d_L2"
      },
      "source": [
        "Suppose that we're in a new universe with unknown light speed $c$, each sample (experiment) of the previous class will give us a *measure* of light speed.\n",
        "You, our explorer, have limited resource to perform experiments, that you can only perform experiments for $100$ times.\n",
        "\n",
        "But you are asked by your commander to give the prediction of $c^2$, which is unexpectingly important in scientific computing.\n",
        "Your rough plan is to design an estimator $Y$ that can give you the prediction, i.e., $\\mathbb{E}\\left[Y\\right] = c^2$.\n",
        "Your commander told you that, *all estimator design begins with writing this form of formula*.\n",
        "\n",
        "After some rough thinking, you think there are three approaches to design the estimator.\n",
        "And you acknowledged that calling `sample` for $n$ times is equivalent to producing $n$ i.i.d. random variables $X_i$ where $\\mathbb{E}\\left[X_i\\right] = c$.\n",
        "For now, you don't have to understand what *biased*, etc. really means.\n",
        "$$\n",
        "\\left\\{\n",
        "\\begin{aligned}\n",
        "  Y_1 &= \\dfrac{1}{100}\\sum_{i = 1}^{100}{X_i^2} &\\text{(biased)} \\\\\n",
        "  Y_2 &= \\left(\\dfrac{1}{100}\\sum_{i = 1}^{100}{X_i}\\right)^2 &\\text{(consistent)} \\\\\n",
        "  Y_3 &= \\left(\\dfrac{1}{50}\\sum_{i = 1}^{50}{X_i}\\right) \\times \\left(\\dfrac{1}{50}\\sum_{i = 51}^{100}{X_i}\\right) &\\text{(unbiased)} \\\\\n",
        "\\end{aligned}\n",
        "\\right.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "668ymeb0d_L2"
      },
      "source": [
        "As an appetizer, your task is to fill the following Python functions with the formulas, which will give us the estimation of $c^2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2VqB7eMd_L3"
      },
      "outputs": [],
      "source": [
        "def biased(normal: Normal) -> float:\n",
        "    result = []\n",
        "    # TODO: your implementation of the *biased* estimator above\n",
        "    for _ in range(100):\n",
        "        pass\n",
        "    return statistics.mean(result)\n",
        "\n",
        "\n",
        "def consistent(normal: Normal) -> float:\n",
        "    result = []\n",
        "    # TODO: your implementation of the *consistent* estimator above\n",
        "    for _ in range(100):\n",
        "        pass\n",
        "    return math.pow(statistics.mean(result), 2)\n",
        "\n",
        "\n",
        "def unbiased(normal: Normal) -> float:\n",
        "    # TODO: your implementation of the *unbiased* estimator above\n",
        "    # First estimator\n",
        "    result = []\n",
        "    for _ in range(50):\n",
        "        pass\n",
        "    I1 = statistics.mean(result)\n",
        "\n",
        "    # Second estimator\n",
        "    result = []\n",
        "    for _ in range(50):\n",
        "        pass\n",
        "    I2 = statistics.mean(result)\n",
        "\n",
        "    return I1 * I2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1JpeRAzd_L3"
      },
      "source": [
        "Great! As a numerical verification, execute the following code,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbA7si3_d_L3"
      },
      "outputs": [],
      "source": [
        "# A sample-based mean/variance estimator\n",
        "def test(func, n: int = 1000) -> float:\n",
        "    result = []\n",
        "    for _ in range(n):\n",
        "        result.append(func())\n",
        "    return (statistics.mean(result), statistics.variance(result))\n",
        "\n",
        "\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "df = pd.DataFrame(index=['biased', 'consistent',\n",
        "                  'unbiased'], columns=['mean', 'variance'])\n",
        "df.loc['biased'] = test(lambda: biased(normal=Normal()))\n",
        "df.loc['consistent'] = test(lambda: consistent(normal=Normal()))\n",
        "df.loc['unbiased'] = test(lambda: unbiased(normal=Normal()))\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEzEALaBd_L4"
      },
      "source": [
        "The performance discrepancy is obvious since the ground truth of $c^2$ is $42$, only the *unbiased* estimator approaches the result.\n",
        "\n",
        "But why is that?\n",
        "\n",
        "Sure, let's take their expectation respectively to observe the result. Recall that,\n",
        "$$\n",
        "  \\dfrac{1}{n}\\sum_{i = 1}^{n} \\mathcal{N}(\\mu_i, \\sigma_i^2) = \\mathcal{N}\\left(\\dfrac{1}{n}\\sum_{i = 1}^{n}{\\mu_i}, \\dfrac{1}{n^2}\\sum_{i = 1}^{n}{\\sigma_i^2}\\right)\n",
        "$$\n",
        "and\n",
        "$$\n",
        "  \\mathbb{V}\\left[X\\right] = \\mathbb{E}\\left[X^2\\right] - \\mathbb{E}^2\\left[X\\right]\n",
        "$$\n",
        "\n",
        "Then let's test our estimators,\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  \\mathbb{E}\\left[Y_1\\right] &= \\dfrac{1}{100}\\sum_{i = 1}^{100}{\\mathbb{E}\\left[X_i^2\\right]} \\\\\n",
        "                             &= \\dfrac{1}{100}\\sum_{i = 1}^{100}{\\left(\\mathbb{V}\\left[X_i\\right] + \\mathbb{E}^2\\left[X_i\\right]\\right)} \\\\\n",
        "                             &=  c^2 + \\dfrac{1}{100}\\sum_{i = 1}^{100}{\\mathbb{V}\\left[X_i\\right]} \\approx 576 + 41 = 617\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-ld6ZQEd_L4"
      },
      "source": [
        "At this point, I believe you have understood what *biased estimator* means, which is, $\\mathbb{E}\\left[{Y_i}\\right] - c^2 > 0$, and no matter how many more samples we obtain, the bias will be pertained, which is the variance of all samples.\n",
        "\n",
        "We could introduce the term *consistent estimator* now. **Asymptotically**, when the number of samples we can obtain converges to $+\\infty$, $\\mathbb{E}\\left[Y_i\\right] = c^2$. Here is the example,\n",
        "Let $X = \\sum_{i} X_i = \\mathcal{N}\\left(\\overline{\\mu}, \\overline{\\sigma^2}/n\\right)$,\n",
        "$$\n",
        "  \\mathbb{E}\\left[Y_2\\right] = \\mathbb{E}\\left[X^2\\right] = \\mathbb{V}\\left[X\\right] + \\mathbb{E}^2\\left[X\\right] = c^2 + \\dfrac{\\sigma^2}{n}\n",
        "$$\n",
        "Then when $n \\rightarrow +\\infty$, the biased is eliminated, which corresponds to the definition of *consistent estimator*.\n",
        "However, whatever the parameter `n` of the `test` function is (not the `n` in `class Normal`, there is a crucial difference between these two $n$, why?), we will not obtain the correct result, since for a specific `n`, the estimator is biased at all.\n",
        "\n",
        "Consistent estimator is great! But we deserve somehow more. For the unbiased estimator, $\\forall n \\ge 1, \\mathbb{E}\\left[Y_3\\right] = c^2$. The proof is yet easy,\n",
        "$$\n",
        "  \\mathbb{E}\\left[Y_3\\right] = \\mathbb{E}\\left[I_1\\right] \\times \\mathbb{E}\\left[I_2\\right] = c^2.\n",
        "$$\n",
        "where $I_1$ and $I_2$ are the independent r.v.s generated from disjoint ranges of $X_i$.\n",
        "\n",
        "Before the end of this section, there is one thing that worth mentioning. *Biased* do not necessarily imply that the estimator is inefficient.\n",
        "Although the previous example's bias is due to inadvertent wrong design.\n",
        "In real cases, sometimes bias are introduced intentionally to obtain low variance,\n",
        "sometimes bias are difficult to mitigate, or it takes a lot of effort to eliminate, so people just leave it as it is.\n",
        "Anyway, you'll meet a lot of biased but efficient estimators along the way, don't panic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-uS94mpd_L4"
      },
      "source": [
        "### Exercise: Unbiased Estimator for Expoential\n",
        "\n",
        "Here's a simple conclusion. Strategy makes a difference. At this point, you should at least understand what are estimators, what are *biased*, *consistent* and *unbiased* estimators.\n",
        "As for designing estimators, which is an extremely difficult and interesting field (not just) in Computer Graphics, I'll present a more interesting example here.\n",
        "\n",
        "Your commander is very satisfied with your estimation, while traveling through a patch of interstellar dust, he asked you to give another *unbiased* estimation to $\\exp(c)$, i.e., $e^c$.\n",
        "Unlike the last time, this time you have more resources to conduct more experiments.\n",
        "\n",
        "In introducing the techniques to solve this problem, we need to retrospect some basics.\n",
        "\n",
        "Suppose we have an estimator (r.v.) $X$. Recall that $X$ is a (measurable) map. Yes, random variables are not variables, they are maps.\n",
        "And there can have a *distribution* defined on its *range*. Since $X$ is a map, and maps can be composed, i.e., $f \\circ g$.\n",
        "Any *valid* functions applied on $X$ can form a new random variable.\n",
        "For example, $f(X)$ is yet another random variable.\n",
        "Further, $X \\cdot Y$ is a new r.v., $p(X)$ is yet r.v., where $p$ is its distribution function.\n",
        "\n",
        "Then, our estimator design for $\\exp(c)$ is $X$, where $\\mathbb{E}\\left[X\\right] = \\exp(c)$. Then, perform Taylor series expansion on $\\exp(c)$.\n",
        "$$\n",
        "  \\exp(c) = 1 + \\dfrac{c}{1!} + \\dfrac{c^2}{2!} + \\dfrac{c^3}{3!} + \\cdots \\; \\text{(countable series)}\n",
        "$$\n",
        "$X$ can be decomposed like this, $\\mathbb{E}\\left[X\\right] = 1 + \\sum_{i = 1}^{\\infty}{\\mathbb{E}\\left[X_i\\right]}$.\n",
        "So our task, actually, is to evaluate this infinite series. But we don't have infinitely many resources to use! Here is where the trick comes.\n",
        "\n",
        "We let $\\mathbb{E}\\left[X_i\\right] = \\mathbb{E}\\left[I_i \\cdot X_i'\\right]$,\n",
        "where $I_i$ is an indicator variable controlling whether this term $X_i$ should be taken into the summation.\n",
        "Since $\\mathbb{E}\\left[I_i \\cdot X_i'\\right] = 1 \\cdot X_i' \\cdot P(I_i = 1)$, $X_i' = X_i P(I_i)^{-1}$.\n",
        "**For any** series of indicator variables $\\{I_i\\}_{i = 1}^{+\\infty}$, this infinite series will evaluate to a correct result.\n",
        "\n",
        "We somehow expect that the series' evaluation process can be terminated in some $i$, then let $K \\sim \\mathrm{Geo}(p)$, and $I_i = [K > i]$, written in [Iverson bracket](https://www.wikiwand.com/en/Iverson%20bracket), we obtain the unbiased estimator of $\\exp(c)$, which is\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  \\mathbb{E}\\left[X\\right] &= \\exp(c) \\\\  \n",
        "    &= 1 + \\sum_{i = 1}^{+\\infty}{I_i \\dfrac{X_i}{P(I_i = 1)}}\n",
        "    = 1 + \\sum_{i = 1}^{K-1}{\\dfrac{X_i}{P(K > i)}}\n",
        "\\end{aligned}\n",
        "$$\n",
        "where $\\mathbb{E}\\left[X_i\\right] = \\dfrac{c^i}{i!}$, whose unbiased estimator design is known from our previous exercise, and $P(K > i) = (1 - p)^i, \\forall i \\ge 1$.\n",
        "\n",
        "Further, one way to obtain (sample) $K$ is to actually simulate the process, where *success* means *termination*. Try to implement it here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYKxfM_Vd_L4"
      },
      "outputs": [],
      "source": [
        "class Normal:\n",
        "    def __init__(self, mu: float) -> None:\n",
        "        self.mu_ = mu\n",
        "        self.sigma_ = 1.0\n",
        "\n",
        "    def sample(self) -> float:\n",
        "        return np.random.normal(self.mu_, self.sigma_)\n",
        "\n",
        "\n",
        "def rr(func) -> float:\n",
        "    p = 0.1       # terminate with a prob 0.1\n",
        "    result = 1.0  # 1 + ... (in Taylor series)\n",
        "    i = 0\n",
        "    while True:\n",
        "        i += 1\n",
        "        if random.random() <= p:\n",
        "            break\n",
        "        result += (func(i) / math.pow((1 - p), i))\n",
        "    return result\n",
        "\n",
        "\n",
        "def exp_series(func, i: int) -> float:\n",
        "    # TODO: fill in your implementation here\n",
        "    result = 1.0\n",
        "    for j in range(i):\n",
        "        pass\n",
        "    return result\n",
        "\n",
        "\n",
        "mu = 2.0\n",
        "print(f'the estimation of [exp({mu})={math.exp(mu):.4f}] is', test(\n",
        "    lambda: rr(lambda i: exp_series(func=Normal(mu).sample, i=i)))[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPJl7Nryd_L5"
      },
      "source": [
        "This approach is called [Russian Roulette](https://www.wikiwand.com/en/Russian_roulette), abbreviate to RR.\n",
        "RR is a such powerful tool to construct estimator on infinite series that you'll see its application later in your rendering homework.\n",
        "\n",
        "In conclusion, we've constructed an unbiased estimator to estimate the form like $\\exp(c)$ with RR.\n",
        "Note that we already have an unbiased estimator for $\\forall i \\ge 1, c^i$.\n",
        "$c$ is not necessarily a constant. We'll see an example later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nI-LAT7Dd_L5"
      },
      "source": [
        "## Monte Carlo Integration\n",
        "\n",
        "Monte Carlo Integration is a realization of *estimator design* and *numerical integration*, which designs estimator on a broad range of integral.\n",
        "To understand how powerful the Monte Carlo Integration is, we start with the definition of *numerical integration*.\n",
        "\n",
        "Unlike our homework helper (*Mathematica*) in the mathematical analysis course, which is symbolic integration, *numerical integration* intends to \"calculate the numerical value of a definite integral\".\n",
        "In 1-D case, we might want,\n",
        "$$\n",
        "  c = \\int_{a}^{b}{f(x) \\, \\mathrm{d} x},\n",
        "$$\n",
        "which can be abstracted to $I_{\\mathrm{numerical}}\\left[\\mathbb{R}, [a, b] \\subset \\mathbb{R}, f\\right] \\rightarrow \\mathbb{R}$, where $f$ might at least defined on $[a, b]$.\n",
        "Note that this function $I_{\\mathrm{numerical}}$ accepts several things, a *set*, the *integral domain* and a *real-valued function* on this domain.\n",
        "\n",
        "A common implementation of $I_{\\mathrm{numerical}}$ is to partition $[a, b]$ into disjoint *subintervals*, then perform some kinds of summations.\n",
        "We'll neglect the detail here because Monte Carlo Integration is somehow completely different.\n",
        "Those who are interested can refer to the explanation of numerical integration on [Wikipedia](https://www.wikiwand.com/en/Numerical_integration).\n",
        "\n",
        "To further introduce Monte Carlo Integration to you, you need to understand why we need it.\n",
        "It is what motivation that drives us away from these simple and intuitive numerical integration approaches?\n",
        "You will be amazed by its *power*, *elegance*, and *simplicity*.\n",
        "\n",
        "Again, Monte Carlo Integration is the realization of *estimator design*. You should ask two questions,\n",
        "- What is the underlying value?\n",
        "- How can we design the estimator?\n",
        "\n",
        "The underlying value, to simplify, is a definite integral where $c = \\int_{a}^{b}{f(x) \\, \\mathrm{d}x}$ (actually, to apply Monte Carlo Integration to Rendering, readers might need to understand integration on general measure space).\n",
        "Recall that we expect an estimator $Y$ such that,\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  \\mathbb{E}\\left[Y\\right] &= c = \\int_{a}^{b}{f(x) \\, \\mathrm{d}x} \\\\\n",
        "    &= \\int_{a'}^{b'}{ y \\cdot p(y) \\, \\mathrm{d}y} \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "to align these two integrals, $a' = a, b' = b$, most importantly,\n",
        "$$\n",
        "  Y = \\dfrac{f(X)}{p(X)} = g(X).\n",
        "$$\n",
        "where $X$ can be **any** valid r.v. To verify,\n",
        "$$\n",
        "  \\mathbb{E}\\left[g(X)\\right] = \\int_{a'}^{b'}\\dfrac{f(x)}{\\cancel{p(x)}} \\cdot \\cancel{p(x)} \\, \\mathrm{d}x.\n",
        "$$\n",
        "Isn't it too simple? But it's this simple method that builds the field *Rendering*.\n",
        "\n",
        "Let's demonstrate this process with code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZDw06X0d_L5"
      },
      "source": [
        "### Exercise: Monte Carlo Estimator\n",
        "\n",
        "You and your fleet are moving through a known stretch of interstellar dust, again!\n",
        "You can your commander realized that you need to perform integration on the density of dust along the way you are traveling to estimate and reduce energy consumption, which is an integral like this,\n",
        "$$\n",
        "  c = \\int_{0}^{4.5}{\\sigma(t) \\, \\mathrm{d}t},\n",
        "$$\n",
        "\n",
        "Random variables from now on are abstracted into a class, which is supposed to be easy to understand.\n",
        "We've provided three examples here, note that `Normal` distribution should not be used as the preceding random variable of Monte Carlo estimator,\n",
        "since it is not bounded, otherwise the estimator is biased.\n",
        "\n",
        "Estimators are generated by factory functions like `make_monte_carlo_estimator`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ecD5v9xd_L5"
      },
      "outputs": [],
      "source": [
        "# https://docs.scipy.org/doc/scipy/tutorial/integrate.html\n",
        "# Use this simple but non-trivial function\n",
        "def func(x): return special.jv(2.5, x)\n",
        "\n",
        "\n",
        "class RandomVariable:\n",
        "    \"\"\"\n",
        "    Definition of RandomVariable\n",
        "        Not a formal definition, but straight-forward\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self): pass\n",
        "    def sample(self) -> float: assert False\n",
        "    def pdf(self, x: float) -> float: assert False\n",
        "\n",
        "    def expect(self, n: int = 1000) -> float:\n",
        "        # LLN\n",
        "        result = 0.0\n",
        "        for _ in range(n):\n",
        "            result += self.sample()\n",
        "        return result / n\n",
        "\n",
        "\n",
        "class Uniform(RandomVariable):\n",
        "    def __init__(self, a: float, b: float):\n",
        "        self.a_ = a\n",
        "        self.b_ = b\n",
        "        self.inv_length_ = 1 / (b - a)\n",
        "\n",
        "    def sample(self) -> float:\n",
        "        return np.random.uniform(low=self.a_, high=self.b_)\n",
        "\n",
        "    def pdf(self, x: float) -> float:\n",
        "        return self.inv_length_\n",
        "\n",
        "\n",
        "class Normal(RandomVariable):\n",
        "    def __init__(self, mu, sigma):\n",
        "        self.mu_ = mu\n",
        "        self.sigma_ = sigma\n",
        "        self.pdf_ = lambda x: norm.pdf(x, self.mu_, self.sigma_)\n",
        "\n",
        "    def sample(self) -> float:\n",
        "        return np.random.normal(self.mu_, self.sigma_)\n",
        "\n",
        "    def pdf(self, x: float) -> float:\n",
        "        return self.pdf_(x)\n",
        "\n",
        "\n",
        "class TruncNormal(RandomVariable):\n",
        "    def __init__(self, a: float, b: float):\n",
        "        self.a_ = a\n",
        "        self.b_ = b\n",
        "        self.mu_ = (a + b) / 2\n",
        "        self.sigma_ = (b - a)\n",
        "        self.cdf_ = lambda x: norm.cdf(x, self.mu_, self.sigma_)\n",
        "        self.pdf_ = lambda x: norm.pdf(x, self.mu_, self.sigma_)\n",
        "        self.factor_ = self.cdf_(b) - self.cdf_(a)\n",
        "\n",
        "    def sample(self) -> float:\n",
        "        # rejection sampling\n",
        "        sample = np.random.normal(self.mu_, self.sigma_)\n",
        "        while not (self.a_ <= sample <= self.b_):\n",
        "            sample = np.random.normal(self.mu_, self.sigma_)\n",
        "        return sample\n",
        "\n",
        "    def pdf(self, x: float) -> float:\n",
        "        return self.pdf_(x) / self.factor_\n",
        "\n",
        "\n",
        "class NumericalIntegral:\n",
        "    \"\"\"\n",
        "    An integral can be abstracted to\n",
        "        1. A function defined on Omega\n",
        "        2. A (measurable) set (indicated by low, high)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, func, low, high) -> None:\n",
        "        self.func_ = func\n",
        "        self.low_ = low\n",
        "        self.high_ = high\n",
        "        self.max_ = None\n",
        "        self.min_ = None\n",
        "\n",
        "    def get_value(self):\n",
        "        # Use general numerical integration method\n",
        "        assert isinstance(self.low_, float) and isinstance(self.high_, float)\n",
        "        return integrate.quad(self.func_, self.low_, self.high_)[0]\n",
        "\n",
        "    def get_max(self):\n",
        "        # Reserved for MCMC methods\n",
        "        assert isinstance(self.low_, float) and isinstance(self.high_, float)\n",
        "        # https://stackoverflow.com/questions/10146924/finding-the-maximum-of-a-function\n",
        "        if self.max_ is None:\n",
        "            self.max_ = -optimize.minimize_scalar(lambda x: -self.func_(\n",
        "                x), bounds=[self.low_, self.high_], method='bounded').fun\n",
        "        return self.max_\n",
        "\n",
        "    def get_min(self):\n",
        "        # Reserved for residual-tracking\n",
        "        assert isinstance(self.low_, float) and isinstance(self.high_, float)\n",
        "        # https://stackoverflow.com/questions/10146924/finding-the-maximum-of-a-function\n",
        "        if self.min_ is None:\n",
        "            self.min_ = optimize.minimize_scalar(\n",
        "                self.func_, bounds=[self.low_, self.high_], method='bounded').fun\n",
        "        return self.min_\n",
        "\n",
        "    def make_monte_carlo_estimator(self, rv_factory) -> RandomVariable:\n",
        "        class Estimator(RandomVariable):\n",
        "            def __init__(self_):\n",
        "                self_.base_rv_ = rv_factory(self.low_, self.high_)\n",
        "\n",
        "            def sample(self_) -> float:\n",
        "                # TODO: fill in your implementation here\n",
        "                # by using .sample() and .pdf() and self.func_()\n",
        "                pass\n",
        "                # return ...\n",
        "        return Estimator()\n",
        "\n",
        "\n",
        "integral = NumericalIntegral(func=func, low=0.0, high=4.5)\n",
        "est_uniform = integral.make_monte_carlo_estimator(Uniform)\n",
        "est_tnormal = integral.make_monte_carlo_estimator(TruncNormal)\n",
        "print(f'groundtruth: {integral.get_value():.4f}')\n",
        "print(f'uniform:     {est_uniform.expect():.4f}')\n",
        "print(f'tnormal:     {est_tnormal.expect():.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_CPeubid_L5"
      },
      "source": [
        "Hey! But things are not finished yet. Monte Carlo *Estimators* are simple, this is what we know. AND, one more thing,\n",
        "\n",
        "**Monte Carlo Integration can be performed on arbitrarily high dimension.**\n",
        "\n",
        "Repeat it for three times, Monte Carlo Integration can be performed on arbitrarily high dimension,\n",
        "Monte Carlo Integration can be performed on arbitrarily high dimension,\n",
        "Monte Carlo Integration can be performed on arbitrarily high dimension.\n",
        "\n",
        "Reflected into the code, this says, the set in $\\mathbb{R}$ indicated by `low` and `high` can be switched into some unknown representation.\n",
        "\n",
        "To avoid spoilers on one of the most the interesting part of Computer Graphics, we'll not delve into this topic.\n",
        "Later, we'll have the chance to utilize this awesome method to imitate the real-world magic in our personal computers!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UCJcWQtd_L6"
      },
      "source": [
        "## Exercise: Unbiased Variance Estimator\n",
        "\n",
        "To test your understanding of the above,\n",
        "we designed a simple exercise involving both Monte Carlo Integration and unbiased estimator design.\n",
        "\n",
        "Here it is! Design an **unbiased** variance estimator and implement it!\n",
        "To be specific, we asked you to design an estimator $\\theta$ for a random variable $X$,\n",
        "so that $\\mathbb{E}\\left[\\theta\\right] = \\mathrm{Var}(X)$.\n",
        "\n",
        "You are given $100$ i.i.d. samples of $X$, $\\left\\{X_i\\right\\}_{i=1}^{100}$.\n",
        "\n",
        "Hint:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  \\mathrm{Var}(X)\n",
        "  & = \\mathbb{E}\\left[X^2\\right] - \\mathbb{E}^2\\left[X\\right] \\\\\n",
        "  & = \\int_{-\\infty}^{\\infty}{x^2 p(x) \\, \\mathrm{d} x} - \\mathbb{E}^2\\left[X\\right].\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Answer:\n",
        "Design $\\theta$ so that (assume $n$ is even)\n",
        "$$\n",
        "  \\theta = \\left[\\dfrac{1}{n} \\sum_{i=1}^{n}X_i^2\\right] -\n",
        "  \\left[\n",
        "    \\left(\\dfrac{2}{n}\\sum_{i=1,3,\\cdots}^{n}X_i\\right)\n",
        "    \\left(\\dfrac{2}{n}\\sum_{i=2,4,\\cdots}^{n}X_i\\right)\n",
        "  \\right]\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnk5G3_7d_L6"
      },
      "outputs": [],
      "source": [
        "def make_variance_estimator(rv: RandomVariable, n: int = 100) -> RandomVariable:\n",
        "    class Estimator(RandomVariable):\n",
        "        def __init__(self, n) -> None:\n",
        "            super().__init__()\n",
        "            self.n_ = n\n",
        "\n",
        "        def sample(self) -> float:\n",
        "            X = [rv.sample() for _ in range(self.n_)]\n",
        "            X = np.array(X)\n",
        "            return 1/self.n_*np.sum(X*X) - \\\n",
        "                (2/self.n_*np.sum(X[:(self.n_//2)])) * \\\n",
        "                (2/self.n_*np.sum(X[(self.n_//2):]))\n",
        "    return Estimator(n)\n",
        "\n",
        "\n",
        "print('normal calculated:', make_variance_estimator(Normal(42, 4)).expect())\n",
        "print('       expected:', 16)\n",
        "print('uniform calculated:', make_variance_estimator(Uniform(0, 1)).expect())\n",
        "print('        expected', 1/12.0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MJAkzQ6d_L6"
      },
      "source": [
        "## Non-linear Estimation using Unbiased-Estimators\n",
        "\n",
        "This section serves as an appendix, to combine the previously mentioned methods together, to estimate this!\n",
        "$$\n",
        "  \\mathrm{Tr}(p \\rightarrow p') = \\exp\\left(-\\int_{0}^{t}{\\sigma(t) \\, \\mathrm{d} t}\\right)\n",
        "$$\n",
        "Which is the transmittance between two points. We might expect an *unbiased* estimator on this.\n",
        "This formula is common in *Volume Rendering*, designing an efficient unbiased estimator of which is generally regarded as a hard task.\n",
        "\n",
        "But, we already have the necessary tools to accomplish this task, just combine the previous functions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SF0ZDOnJd_L6"
      },
      "outputs": [],
      "source": [
        "def make_taylor_transmittance_estimator(integral: NumericalIntegral):\n",
        "    estimator = integral.make_monte_carlo_estimator(Uniform)\n",
        "\n",
        "    class Estimator(RandomVariable):\n",
        "        def __init__(self) -> None:\n",
        "            super().__init__()\n",
        "\n",
        "        def sample(self) -> float:\n",
        "            return rr(lambda i: exp_series(func=estimator.sample, i=i))\n",
        "    return Estimator()\n",
        "\n",
        "\n",
        "m_integral = NumericalIntegral(func=lambda x: -func(x), low=0.0, high=4.5)\n",
        "tr_est_uniform = make_taylor_transmittance_estimator(m_integral)\n",
        "print(f'groundtruth: {np.exp(m_integral.get_value()):.4f}')\n",
        "print(f'mean:        {tr_est_uniform.expect():.4f}')\n",
        "print(f'variance:    {make_variance_estimator(tr_est_uniform, 10).expect():.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qpa93e10d_L6"
      },
      "source": [
        "## MCMC Methods\n",
        "\n",
        "This part is completely optional, but we recommend you to execute it and check its result towards the previous method.\n",
        "It is based on [this paper](https://cs.dartmouth.edu/wjarosz/publications/novak14residual.pdf),\n",
        "you can refer to it for pseudo-code for further study.\n",
        "\n",
        "Just execute this code! Experience the power of MCMC methods. You don't need to understand the algorithm right now,\n",
        "but we strongly recommend that you read the relevant papers afterwards. The idea, intuition and derivation of this series of methods is ingenious."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WltVx-QXd_L6"
      },
      "outputs": [],
      "source": [
        "def delta_tracking(integral: NumericalIntegral, max_f: float):\n",
        "    x = integral.low_\n",
        "    while True:\n",
        "        eta = np.random.uniform()\n",
        "        x -= np.log(1 - eta) / max_f\n",
        "        if x >= integral.high_:\n",
        "            break\n",
        "        epsilon = np.random.uniform()\n",
        "        if epsilon <= integral.func_(x) / max_f:\n",
        "            break\n",
        "    return x\n",
        "\n",
        "\n",
        "def make_delta_tracking_transmittance_estimator(integral: NumericalIntegral) -> RandomVariable:\n",
        "    max_f = integral.get_max()\n",
        "\n",
        "    class Estimator(RandomVariable):\n",
        "        def __init__(self) -> None:\n",
        "            super().__init__()\n",
        "\n",
        "        def sample(self) -> float:\n",
        "            return 1 if delta_tracking(integral, max_f) > integral.high_ else 0\n",
        "    return Estimator()\n",
        "\n",
        "\n",
        "def make_ratio_tracking_transmittance_estimator(integral: NumericalIntegral) -> RandomVariable:\n",
        "    max_f = integral.get_max()\n",
        "\n",
        "    class Estimator(RandomVariable):\n",
        "        def __init__(self) -> None:\n",
        "            super().__init__()\n",
        "\n",
        "        def sample(self) -> float:\n",
        "            x = integral.low_\n",
        "            T = 1\n",
        "            while True:\n",
        "                eta = np.random.uniform()\n",
        "                x -= np.log(1 - eta) / max_f\n",
        "                if x >= integral.high_:\n",
        "                    break\n",
        "                T *= (1 - integral.func_(x) / max_f)\n",
        "            return T\n",
        "    return Estimator()\n",
        "\n",
        "\n",
        "def make_residual_tracking_transmittance_estimator(integral: NumericalIntegral, f_c: float) -> RandomVariable:\n",
        "    class Estimator(RandomVariable):\n",
        "        def __init__(self, f_c) -> None:\n",
        "            super().__init__()\n",
        "            self.f_c_ = f_c\n",
        "            self.f_r_ = -optimize.minimize_scalar(\n",
        "                lambda x: -np.abs(integral.func_(x) - f_c), bounds=[integral.low_, integral.high_], method='bounded').fun\n",
        "\n",
        "        def sample(self) -> float:\n",
        "            x = integral.low_\n",
        "            Tc = np.exp(-self.f_c_ * (integral.high_ - integral.low_))\n",
        "            Tr = 1.0\n",
        "            while True:\n",
        "                eta = np.random.uniform()\n",
        "                x -= np.log(1 - eta) / self.f_r_\n",
        "                if x >= integral.high_:\n",
        "                    break\n",
        "                Tr *= 1 - (integral.func_(x) - self.f_c_) / self.f_r_\n",
        "            return Tc * Tr\n",
        "    return Estimator(f_c)\n",
        "\n",
        "\n",
        "tr_est_dt = make_delta_tracking_transmittance_estimator(integral)\n",
        "tr_est_rt = make_ratio_tracking_transmittance_estimator(integral)\n",
        "tr_est_rst_mean = make_residual_tracking_transmittance_estimator(\n",
        "    integral, integral.get_value() / (integral.high_ - integral.low_))\n",
        "print(f'groundtruth:  {np.exp(m_integral.get_value()):.4f}')\n",
        "print(f'our mean:     {tr_est_uniform.expect():.4f}')\n",
        "print(f'dt  mean:     {tr_est_dt.expect():.4f}')\n",
        "print(f'rt  mean:     {tr_est_rt.expect():.4f}')\n",
        "print(f'rst mean:     {tr_est_rst_mean.expect():.4f}')\n",
        "print(\n",
        "    f'our variance: {make_variance_estimator(tr_est_uniform, 10).expect():.4f}')\n",
        "print(f'dt  variance: {make_variance_estimator(tr_est_dt, 10).expect():.4f}')\n",
        "print(f'rt  variance: {make_variance_estimator(tr_est_rt, 10).expect():.4f}')\n",
        "print(\n",
        "    f'rst variance: {make_variance_estimator(tr_est_rst_mean, 10).expect():.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsgRAkGCd_L6"
      },
      "source": [
        "Their implementations seem easy. Well, yes!\n",
        "But the derivation is rather complex, and requires you to manage the previous concepts (basic college math is sufficient for you to read this part).\n",
        "You can refer to [this material](https://cs.dartmouth.edu/wjarosz/publications/novak14residual-supplemental1.pdf) for more information.\n",
        "It further includes the proof of residual tracking, which is, again, the application of [control variates](https://www.wikiwand.com/en/Control_variates).\n",
        "We'll only prove *Ratio-Tracking* here for you to entertain ;)\n",
        "\n",
        "Happy grinding, and welcome to Computer Graphics!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFSPs5lBd_L6"
      },
      "source": [
        "### Proof of Ratio-Tracking\n",
        "\n",
        "To recap, we are creating unbiased estimator for (simplified to $[0, d]$)\n",
        "$$\n",
        "T_r = \\exp\\left(-\\int_{0}^{d} \\mu(x) \\, \\mathrm{d}x\\right)\n",
        "$$\n",
        "That is to say, we are designing an unbiased estimator $T$, such that $\\mathbb{E}\\left[T\\right] = T_r$. $S_i$ are some i.i.d. random variables satisfying $S_i \\sim \\mathrm{Exp}(\\overline{\\mu})$, and $C_i = \\sum_{j=1}^{i} S_j$, yet some random variables. Let $K$ be the maximum $i$ for $C_i \\le d$. This is a random process.\n",
        "\n",
        "Since $\\mathbb{E}\\left[T\\right] = \\mathbb{E}_{k \\sim K}\\left[\\mathbb{E}\\left[T \\mid k\\right]\\right]$, suppose $T_k = T \\mid k$, then\n",
        "$$\n",
        "\\mathbb{E}\\left[T\\right] = \\sum_{k=0}^{\\infty}\\mathbb{E}\\left[T_k\\right]P(K=k).\n",
        "$$\n",
        "We derive some terms with Mathematica, let $R_k = \\mathbb{E}\\left[T_k\\right] P(K = k)$. Then\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\tR_k\n",
        "\t& = \\int_{\\Omega_k} T_k P(\\cdot \\mid K = k) P(K = k) \\\\\n",
        "\t& = \\int_{\\Omega_k} T_k P(S_i \\text{ satisfying some conds}),\n",
        "\\end{aligned}\n",
        "$$\n",
        "where $\\Omega_k \\in \\mathbb{R}^k$.\n",
        "\n",
        "We find that $R_0 = \\exp{\\left(-d \\mu\\right)} \\cdot 1$, $R = d \\exp{\\left(-d \\mu\\right)} \\mu \\cdot $, and... (don't panic! this is not difficult. We are just destructing $P\\left(\\sum_{i=1}^{k}{S_i} \\le d, \\sum_{i=1}^{k+1}{S_i} > d\\right) \\prod_{i=1}^{k}{\\iota\\left(\\sum_{j=1}^{i}{S_j}\\right)}$.\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\tR_k\n",
        "\t& = \\int_{0}^{d} p_S(x_1) \\iota(x_1) \\int_{0}^{d - x_1} p_S(x_2) \\iota(x_1 + x_2) \\cdots \\\\\n",
        "\t& \\cdots \\int_{0}^{d - \\sum_{j=1}^{k-1}{x_j}} p_S(x_k) \\iota\\left(\\sum_{j=1}^{k}x_j\\right) \\\\\n",
        "\t& \\times \\int_{d - \\sum_{j=1}^{k-1}{x_j}}^{\\infty} p_S(x_{k+1}) \\, \\mathrm{d} \\Omega_k \\\\\n",
        "\t& \\xrightarrow{\\text{some magic}} \\overline{\\mu}^k \\exp{\\left(- \\mu d\\right)} \\cdot \\int_{0}^{d} \\iota(z_1) \\int_{0}^{d - z_1} \\iota(z_2) \\cdots ,\n",
        "\\end{aligned}\n",
        "$$\n",
        "where $z_i$ are $\\sum_{j=1}^{i} x_j$. The *magic* is easily obtained by just multiplying the PDFs together.\n",
        "\n",
        "Here's a non-trivial transformation that worth mentioning, how can we deal with (note that the equation 13 in the original supplementary material is incorrect, [this](https://math.stackexchange.com/questions/3778215/multiple-integral-over-a-k-dimensional-simplex) is correct ;)\n",
        "$$\n",
        "\\int_{0}^{d} \\int_{z_1}^{d} \\cdots \\prod_{j=1}^{k}{\\iota(z_j)}\n",
        "$$\n",
        "Notice that $\\iota(\\mathbf{z})$ is symmetry over all dimensions, and we're necessarily integrating some $k$-dimensional [simplex](https://www.wikiwand.com/en/Simplex) (you understand it when you draw it). It can be transformed into\n",
        "$$\n",
        "R_k = \\overline{\\mu}^k \\exp{\\left(- \\overline{\\mu} d\\right)} \\left(\\int_{0}^{d}{\\iota(x) \\, \\mathrm{d} x}\\right)^k {\\Large{/}} k!.\n",
        "$$\n",
        "This equation can be easily understood in $k = 2$ using symmetry.\n",
        "\n",
        "Then\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\t\\mathbb{E}\\left[T\\right]\n",
        "\t& = \\sum_{k=0}^{\\infty}R_k = \\exp(-\\overline{\\mu} d) \\cdot \\exp{\\left(\\overline{\\mu} \\int_{0}^{d} 1 - \\dfrac{\\mu(x)}{\\overline{\\mu}} \\, \\mathrm{d} x\\right)} \\\\\n",
        "\t& = \\exp{\\left(- \\int_{0}^{d} \\mu(x) \\, \\mathrm{d} x\\right)}\n",
        "\\end{aligned}\n",
        "$$\n",
        "Anyway, the core idea is to construct a form like,\n",
        "$$\n",
        "\\sum_{k} \\left(\\int_{0}^{d} -\\mu(x) \\, \\mathrm{d} x\\right)^k {\\Large/} k!\n",
        "$$\n",
        "though the approach is quite tricky."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOZ0K19vd_L7"
      },
      "source": [
        "## Epilogue\n",
        "\n",
        "We have to admit, the topic of *rendering* is far from over, and the math is already becoming harder.\n",
        "But one thing to mention here, so far, the math is *concrete*. We are performing integrals in trivial sets or ranges.\n",
        "Once you figure out the concept through those (elaborately designed) interfaces, we can proceed to the next level,\n",
        "to grind through those abstractly defined equations and integrals!\n",
        "This is where the difficulty for neophyte in rendering comes from.\n",
        "\n",
        "Please notice, we are entering a brand new mathematically defined world after this lab, not the one that you are familiar with.\n",
        "We encourage you to ask yourself questions, for example, random variables are defined on *sample space*, so how do we design an estimator on this integral;\n",
        "what is **dummy variable** in integral; what do we really mean by integrating on solid angle?\n",
        "To build a comprehensive and mathematically correct path tracer, you should be able to ask and answer these questions.\n",
        "If you encounter any difficulties, reflect your model of abstraction, and we are happy to discuss these things with you!\n",
        "\n",
        "There are some good resources that worth reading. One we'd like to recommend here, aside from Physically Based Rendering, is\n",
        "ROBUST MONTE CARLO METHODS FOR LIGHT TRANSPORT SIMULATION. Check this [meme](https://twitter.com/yiningkarlli/status/1545186689894584320) :)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}